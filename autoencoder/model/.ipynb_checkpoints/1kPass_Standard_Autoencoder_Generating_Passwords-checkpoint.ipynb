{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the 1000 passwords dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPZ1jrOMhqZa",
    "outputId": "08d09717-9fa0-4b34-a422-f2105ae28417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "    Capital   Word Numeric Special FullPassword\n",
      "0         A   cafe    2019       !   Acafe2019!\n",
      "1         A  hyper    2019       !  Ahyper2019!\n",
      "2         P   leet      10       !     Pleet10!\n",
      "3         A   babe       2       !      Ababe2!\n",
      "4         T  bingo    1234       *  Tbingo1234*\n",
      "..      ...    ...     ...     ...          ...\n",
      "995       A   leet     123       !    Aleet123!\n",
      "996       A  hyper     777       !   Ahyper777!\n",
      "997       P   cafe     123       *    Pcafe123*\n",
      "998       A   cafe      10       %     Acafe10%\n",
      "999       A   babe     101       _    Ababe101_\n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/Shareddrives/ENLP Final Projects - Passwords/autoencoder/\"\n",
    "\n",
    "passwords = pd.read_csv(path + \"data/1000passwords.csv\", dtype={'Numeric': str})\n",
    "print(passwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Character Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hQpuqedCmCK",
    "outputId": "acfacb56-d919-43d4-aa49-1f2e2d064cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass length: 15\n",
      "Vocab size: 38\n",
      "Encoded passwords shape: (1000, 15)\n",
      "One-hot encoded passwords shape: (1000, 15)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Identify max password length in dataset and pad rest of the passwords such that all of them have the same length.\n",
    "# Haveing same length sequences is a requirement for LSTM\n",
    "PAD_CHAR = \"~\"\n",
    "PASS_LENGTH = max([len(p) for p in passwords[\"FullPassword\"]])\n",
    "\n",
    "padded_passwords = []\n",
    "charset = set(PAD_CHAR)               # start with the initial padding char\n",
    "for p in passwords[\"FullPassword\"]:\n",
    "  padded_passwords.append(p.ljust(PASS_LENGTH, PAD_CHAR))\n",
    "  charset |= set(p)                   # |= is the union set operation.\n",
    "\n",
    "# Convert characters to integers \n",
    "vocab_size = len(charset)\n",
    "char2id = dict((c, i) for i, c in enumerate(charset))\n",
    "\n",
    "# One hot encode the passwords\n",
    "encoded_passwords = [[char2id[c] for c in password] for password in padded_passwords]\n",
    "one_hot_encoded = np.array([to_categorical(p, num_classes=vocab_size) for p in encoded_passwords])\n",
    "\n",
    "print(\"Pass length: \" + str(PASS_LENGTH))\n",
    "print(\"Vocab size: \" + str(vocab_size))\n",
    "print(\"Encoded passwords shape: \" + str(np.shape(encoded_passwords)))\n",
    "print(\"One-hot encoded passwords shape: \" + str(np.shape(encoded_passwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-opwLcs1tok"
   },
   "source": [
    "The Standard Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xxqO0l8JGox"
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def create_lstm_autoencoder(timesteps, layer_sizes, vocab_size):\n",
    "  \"\"\"\n",
    "  Creates a symmetric LSTM autoencoder model and returns, the autoencoder model, encoder model,\n",
    "  and decoder model to be used individually.\n",
    "  \"\"\"\n",
    "  # Create encoder model\n",
    "  enc_input = Input(shape=(timesteps, vocab_size))\n",
    "  x = enc_input\n",
    "  for idx, layer_size in enumerate(layer_sizes):\n",
    "    ret_seq = (idx != len(layer_sizes) - 1) # False for the last layer_size\n",
    "    x = Bidirectional(LSTM(layer_size, return_sequences=ret_seq))(x)\n",
    "  enc_output = Dense(layer_sizes[-1], activation=\"relu\")(x)\n",
    "  encoder = Model(enc_input, enc_output, name=\"Encoder\")\n",
    "\n",
    "  # Create decoder model\n",
    "  bottleneck_size = layer_sizes[-1]\n",
    "  dec_input = Input((bottleneck_size,))\n",
    "  x = RepeatVector(timesteps)(dec_input)\n",
    "  for layer_size in layer_sizes[::-1][1:]:\n",
    "    x = Bidirectional(LSTM(layer_size, return_sequences=True))(x)\n",
    "  dec_output = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(x)\n",
    "  decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "\n",
    "  # Connect decoder with encoder\n",
    "  connected_decoder = decoder(enc_output)\n",
    "\n",
    "  # Create autoencoder model\n",
    "  autoencoder = Model(enc_input, connected_decoder, name=\"Autoencoder\")\n",
    "  autoencoder.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "  return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEZqYQI2U6ai",
    "outputId": "b8f4aada-ee9a-4f28-a3ae-0b8b297c7275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 15, 38)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 15, 32)            7040      \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 15, 20)            3440      \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 12)                1296      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 78        \n",
      "=================================================================\n",
      "Total params: 11,854\n",
      "Trainable params: 11,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 15, 6)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 15, 20)            1360      \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 15, 32)            4736      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 38)            1254      \n",
      "=================================================================\n",
      "Total params: 7,350\n",
      "Trainable params: 7,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, autoencoder = create_lstm_autoencoder(PASS_LENGTH, [16, 10, 6], vocab_size)\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcT_kWQOYV8K",
    "outputId": "93c65e3e-e7e8-415b-9d9b-9fd6078a6a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "100/100 [==============================] - 41s 16ms/step - loss: 3.2912 - accuracy: 0.2972\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 2.2299 - accuracy: 0.4458\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.8405 - accuracy: 0.4758\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 1.6540 - accuracy: 0.4915\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.5286 - accuracy: 0.5090\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.4194 - accuracy: 0.5481\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.3512 - accuracy: 0.5729\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.2847 - accuracy: 0.6071\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.2122 - accuracy: 0.6273\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.1523 - accuracy: 0.6464\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.1031 - accuracy: 0.6574\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.0436 - accuracy: 0.6796\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 1.0077 - accuracy: 0.6945\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.9643 - accuracy: 0.7041\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.9206 - accuracy: 0.7202\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.8855 - accuracy: 0.7233\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.8299 - accuracy: 0.7541\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.7945 - accuracy: 0.7707\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.7548 - accuracy: 0.7751\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.7101 - accuracy: 0.7900\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6974 - accuracy: 0.7946\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6521 - accuracy: 0.7995\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6152 - accuracy: 0.8210\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6103 - accuracy: 0.8145\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.6033 - accuracy: 0.8104\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.5650 - accuracy: 0.8244\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.5466 - accuracy: 0.8250\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.5473 - accuracy: 0.8251\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.5185 - accuracy: 0.8330\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4959 - accuracy: 0.8439\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4884 - accuracy: 0.8445\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4761 - accuracy: 0.8503\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4666 - accuracy: 0.8540\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4489 - accuracy: 0.8614\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4508 - accuracy: 0.8594\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4219 - accuracy: 0.8754\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4251 - accuracy: 0.8716\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4053 - accuracy: 0.8773\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3878 - accuracy: 0.8790\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3810 - accuracy: 0.8829\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3826 - accuracy: 0.8809\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3573 - accuracy: 0.8886\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3494 - accuracy: 0.8918\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3407 - accuracy: 0.8974\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3379 - accuracy: 0.8942\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3728 - accuracy: 0.8829\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3330 - accuracy: 0.8937\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3182 - accuracy: 0.8999\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3106 - accuracy: 0.9023\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3221 - accuracy: 0.8960\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3116 - accuracy: 0.8981\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3002 - accuracy: 0.9048\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2882 - accuracy: 0.9068\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2878 - accuracy: 0.9074\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2912 - accuracy: 0.9038\n",
      "Epoch 56/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3497 - accuracy: 0.8862\n",
      "Epoch 57/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2726 - accuracy: 0.9108\n",
      "Epoch 58/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2689 - accuracy: 0.9089\n",
      "Epoch 59/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2647 - accuracy: 0.9109\n",
      "Epoch 60/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2586 - accuracy: 0.9130\n",
      "Epoch 61/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2615 - accuracy: 0.9103\n",
      "Epoch 62/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3241 - accuracy: 0.8877\n",
      "Epoch 63/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2947 - accuracy: 0.8982\n",
      "Epoch 64/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2553 - accuracy: 0.9094\n",
      "Epoch 65/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2488 - accuracy: 0.9117\n",
      "Epoch 66/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2458 - accuracy: 0.9122\n",
      "Epoch 67/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2460 - accuracy: 0.9127\n",
      "Epoch 68/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2428 - accuracy: 0.9126\n",
      "Epoch 69/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2567 - accuracy: 0.9103\n",
      "Epoch 70/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2345 - accuracy: 0.9165\n",
      "Epoch 71/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2344 - accuracy: 0.9171\n",
      "Epoch 72/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2364 - accuracy: 0.9152\n",
      "Epoch 73/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2938 - accuracy: 0.8945\n",
      "Epoch 74/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2336 - accuracy: 0.9140\n",
      "Epoch 75/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2256 - accuracy: 0.9187\n",
      "Epoch 76/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2257 - accuracy: 0.9190\n",
      "Epoch 77/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2253 - accuracy: 0.9181\n",
      "Epoch 78/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2338 - accuracy: 0.9144\n",
      "Epoch 79/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2237 - accuracy: 0.9166\n",
      "Epoch 80/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2897 - accuracy: 0.8942\n",
      "Epoch 81/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2344 - accuracy: 0.9137\n",
      "Epoch 82/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2200 - accuracy: 0.9185\n",
      "Epoch 83/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2202 - accuracy: 0.9155\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2134 - accuracy: 0.9206\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2079 - accuracy: 0.9226\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2200 - accuracy: 0.9165\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2126 - accuracy: 0.9195\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2194 - accuracy: 0.9177\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1999 - accuracy: 0.9230\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2081 - accuracy: 0.9185\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1970 - accuracy: 0.9247\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2026 - accuracy: 0.9209\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1949 - accuracy: 0.9243\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2107 - accuracy: 0.9178\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2077 - accuracy: 0.9207\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2721 - accuracy: 0.8943\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2020 - accuracy: 0.9205\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1951 - accuracy: 0.9240\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1842 - accuracy: 0.9284\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1873 - accuracy: 0.9294\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2076 - accuracy: 0.9182\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2344 - accuracy: 0.9112\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1943 - accuracy: 0.9251\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1830 - accuracy: 0.9294\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1793 - accuracy: 0.9345\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1772 - accuracy: 0.9343\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1789 - accuracy: 0.9347\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1743 - accuracy: 0.9359\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2142 - accuracy: 0.9197\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1892 - accuracy: 0.9265\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1721 - accuracy: 0.9354\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1697 - accuracy: 0.9363\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1678 - accuracy: 0.9372\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1781 - accuracy: 0.9321\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1664 - accuracy: 0.9386\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1661 - accuracy: 0.9399\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1646 - accuracy: 0.9381\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1639 - accuracy: 0.9404\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1602 - accuracy: 0.9396\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1611 - accuracy: 0.9392\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1690 - accuracy: 0.9335\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.3126 - accuracy: 0.8841\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1827 - accuracy: 0.9287\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1666 - accuracy: 0.9377\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1635 - accuracy: 0.9372\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1584 - accuracy: 0.9395\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1599 - accuracy: 0.9393\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1533 - accuracy: 0.9433\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1547 - accuracy: 0.9414\n",
      "Epoch 130/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1872 - accuracy: 0.9275\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2050 - accuracy: 0.9199\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1627 - accuracy: 0.9368\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1552 - accuracy: 0.9396\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1521 - accuracy: 0.9429\n",
      "Epoch 135/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1506 - accuracy: 0.9452\n",
      "Epoch 136/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1533 - accuracy: 0.9422\n",
      "Epoch 137/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1505 - accuracy: 0.9430\n",
      "Epoch 138/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1497 - accuracy: 0.9431\n",
      "Epoch 139/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1501 - accuracy: 0.9423\n",
      "Epoch 140/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1703 - accuracy: 0.9325\n",
      "Epoch 141/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1552 - accuracy: 0.9400\n",
      "Epoch 142/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1514 - accuracy: 0.9400\n",
      "Epoch 143/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1545 - accuracy: 0.9408\n",
      "Epoch 144/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1665 - accuracy: 0.9374\n",
      "Epoch 145/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1872 - accuracy: 0.9257\n",
      "Epoch 146/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2036 - accuracy: 0.9195\n",
      "Epoch 147/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1515 - accuracy: 0.9396\n",
      "Epoch 148/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1461 - accuracy: 0.9432\n",
      "Epoch 149/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1451 - accuracy: 0.9426\n",
      "Epoch 150/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1419 - accuracy: 0.9461\n",
      "Epoch 151/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1427 - accuracy: 0.9464\n",
      "Epoch 152/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1408 - accuracy: 0.9455\n",
      "Epoch 153/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1475 - accuracy: 0.9406\n",
      "Epoch 154/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1750 - accuracy: 0.9329\n",
      "Epoch 155/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1500 - accuracy: 0.9399\n",
      "Epoch 156/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1448 - accuracy: 0.9456\n",
      "Epoch 157/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1422 - accuracy: 0.9431\n",
      "Epoch 158/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1391 - accuracy: 0.9469\n",
      "Epoch 159/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2189 - accuracy: 0.9182\n",
      "Epoch 160/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1759 - accuracy: 0.9345\n",
      "Epoch 161/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1431 - accuracy: 0.9446\n",
      "Epoch 162/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1387 - accuracy: 0.9472\n",
      "Epoch 163/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1379 - accuracy: 0.9473\n",
      "Epoch 164/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1361 - accuracy: 0.9488\n",
      "Epoch 165/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1370 - accuracy: 0.9464\n",
      "Epoch 166/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1379 - accuracy: 0.9457\n",
      "Epoch 167/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1434 - accuracy: 0.9433\n",
      "Epoch 168/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1402 - accuracy: 0.9471\n",
      "Epoch 169/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1680 - accuracy: 0.9349\n",
      "Epoch 170/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2108 - accuracy: 0.9186\n",
      "Epoch 171/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1617 - accuracy: 0.9377\n",
      "Epoch 172/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1344 - accuracy: 0.9502\n",
      "Epoch 173/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1375 - accuracy: 0.9451\n",
      "Epoch 174/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1313 - accuracy: 0.9501\n",
      "Epoch 175/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1302 - accuracy: 0.9486\n",
      "Epoch 176/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1296 - accuracy: 0.9507\n",
      "Epoch 177/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1286 - accuracy: 0.9511\n",
      "Epoch 178/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1292 - accuracy: 0.9494\n",
      "Epoch 179/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1333 - accuracy: 0.9484\n",
      "Epoch 180/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1403 - accuracy: 0.9445\n",
      "Epoch 181/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1321 - accuracy: 0.9474\n",
      "Epoch 182/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1300 - accuracy: 0.9474\n",
      "Epoch 183/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1471 - accuracy: 0.9421\n",
      "Epoch 184/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1786 - accuracy: 0.9277\n",
      "Epoch 185/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1287 - accuracy: 0.9484\n",
      "Epoch 186/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1221 - accuracy: 0.9536\n",
      "Epoch 187/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1241 - accuracy: 0.9527\n",
      "Epoch 188/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1292 - accuracy: 0.9507\n",
      "Epoch 189/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1265 - accuracy: 0.9513\n",
      "Epoch 190/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1510 - accuracy: 0.9399\n",
      "Epoch 191/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1219 - accuracy: 0.9528\n",
      "Epoch 192/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1260 - accuracy: 0.9532\n",
      "Epoch 193/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2236 - accuracy: 0.9200\n",
      "Epoch 194/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2084 - accuracy: 0.9164\n",
      "Epoch 195/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1431 - accuracy: 0.9428\n",
      "Epoch 196/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1251 - accuracy: 0.9506\n",
      "Epoch 197/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1214 - accuracy: 0.9521\n",
      "Epoch 198/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1187 - accuracy: 0.9528\n",
      "Epoch 199/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1205 - accuracy: 0.9540\n",
      "Epoch 200/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1179 - accuracy: 0.9529\n",
      "Epoch 201/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1191 - accuracy: 0.9535\n",
      "Epoch 202/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1422 - accuracy: 0.9426\n",
      "Epoch 203/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1192 - accuracy: 0.9537\n",
      "Epoch 204/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1189 - accuracy: 0.9529\n",
      "Epoch 205/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1143 - accuracy: 0.9555\n",
      "Epoch 206/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1158 - accuracy: 0.9543\n",
      "Epoch 207/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1146 - accuracy: 0.9547\n",
      "Epoch 208/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1130 - accuracy: 0.9560\n",
      "Epoch 209/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1129 - accuracy: 0.9568\n",
      "Epoch 210/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1309 - accuracy: 0.9463\n",
      "Epoch 211/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1599 - accuracy: 0.9358\n",
      "Epoch 212/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1854 - accuracy: 0.9283\n",
      "Epoch 213/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1134 - accuracy: 0.9555\n",
      "Epoch 214/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1107 - accuracy: 0.9564\n",
      "Epoch 215/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1091 - accuracy: 0.9557\n",
      "Epoch 216/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1085 - accuracy: 0.9578\n",
      "Epoch 217/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1086 - accuracy: 0.9568\n",
      "Epoch 218/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1078 - accuracy: 0.9576\n",
      "Epoch 219/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1057 - accuracy: 0.9594\n",
      "Epoch 220/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1137 - accuracy: 0.9559\n",
      "Epoch 221/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1057 - accuracy: 0.9574\n",
      "Epoch 222/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1958 - accuracy: 0.9279\n",
      "Epoch 223/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1193 - accuracy: 0.9522\n",
      "Epoch 224/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1084 - accuracy: 0.9561\n",
      "Epoch 225/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1071 - accuracy: 0.9567\n",
      "Epoch 226/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1038 - accuracy: 0.9587\n",
      "Epoch 227/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1049 - accuracy: 0.9594\n",
      "Epoch 228/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1266 - accuracy: 0.9485\n",
      "Epoch 229/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1089 - accuracy: 0.9557\n",
      "Epoch 230/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1063 - accuracy: 0.9562\n",
      "Epoch 231/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1068 - accuracy: 0.9577\n",
      "Epoch 232/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1018 - accuracy: 0.9581\n",
      "Epoch 233/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1020 - accuracy: 0.9598\n",
      "Epoch 234/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1623 - accuracy: 0.9425\n",
      "Epoch 235/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1216 - accuracy: 0.9510\n",
      "Epoch 236/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1123 - accuracy: 0.9550\n",
      "Epoch 237/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1047 - accuracy: 0.9591\n",
      "Epoch 238/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1046 - accuracy: 0.9573\n",
      "Epoch 239/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1026 - accuracy: 0.9608\n",
      "Epoch 240/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1020 - accuracy: 0.9603\n",
      "Epoch 241/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1062 - accuracy: 0.9571\n",
      "Epoch 242/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1055 - accuracy: 0.9562\n",
      "Epoch 243/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1022 - accuracy: 0.9583\n",
      "Epoch 244/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.2087 - accuracy: 0.9237\n",
      "Epoch 245/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1214 - accuracy: 0.9517\n",
      "Epoch 246/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1012 - accuracy: 0.9586\n",
      "Epoch 247/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1001 - accuracy: 0.9595\n",
      "Epoch 248/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0980 - accuracy: 0.9609\n",
      "Epoch 249/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0974 - accuracy: 0.9610\n",
      "Epoch 250/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1047 - accuracy: 0.9557\n",
      "Epoch 251/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1008 - accuracy: 0.9592\n",
      "Epoch 252/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1005 - accuracy: 0.9591\n",
      "Epoch 253/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0966 - accuracy: 0.9622\n",
      "Epoch 254/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0974 - accuracy: 0.9608\n",
      "Epoch 255/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1002 - accuracy: 0.9607\n",
      "Epoch 256/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.2410 - accuracy: 0.9120\n",
      "Epoch 257/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1174 - accuracy: 0.9519\n",
      "Epoch 258/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0995 - accuracy: 0.9584\n",
      "Epoch 259/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0966 - accuracy: 0.9634\n",
      "Epoch 260/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0954 - accuracy: 0.9612\n",
      "Epoch 261/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0922 - accuracy: 0.9641\n",
      "Epoch 262/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0918 - accuracy: 0.9647\n",
      "Epoch 263/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1029 - accuracy: 0.9596\n",
      "Epoch 264/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1012 - accuracy: 0.9583\n",
      "Epoch 265/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0923 - accuracy: 0.9653\n",
      "Epoch 266/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0932 - accuracy: 0.9646\n",
      "Epoch 267/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1155 - accuracy: 0.9559\n",
      "Epoch 268/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1528 - accuracy: 0.9377\n",
      "Epoch 269/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0934 - accuracy: 0.9640\n",
      "Epoch 270/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0965 - accuracy: 0.9609\n",
      "Epoch 271/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0936 - accuracy: 0.9645\n",
      "Epoch 272/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0929 - accuracy: 0.9651\n",
      "Epoch 273/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0914 - accuracy: 0.9636\n",
      "Epoch 274/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0924 - accuracy: 0.9651\n",
      "Epoch 275/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0891 - accuracy: 0.9684\n",
      "Epoch 276/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1059 - accuracy: 0.9591\n",
      "Epoch 277/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1030 - accuracy: 0.9580\n",
      "Epoch 278/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0903 - accuracy: 0.9667\n",
      "Epoch 279/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0915 - accuracy: 0.9655\n",
      "Epoch 280/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0934 - accuracy: 0.9629\n",
      "Epoch 281/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1180 - accuracy: 0.9542\n",
      "Epoch 282/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1114 - accuracy: 0.9554\n",
      "Epoch 283/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0944 - accuracy: 0.9626\n",
      "Epoch 284/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0895 - accuracy: 0.9647\n",
      "Epoch 285/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0902 - accuracy: 0.9669\n",
      "Epoch 286/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1041 - accuracy: 0.9599\n",
      "Epoch 287/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0876 - accuracy: 0.9680\n",
      "Epoch 288/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1216 - accuracy: 0.9530\n",
      "Epoch 289/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1533 - accuracy: 0.9385\n",
      "Epoch 290/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0904 - accuracy: 0.9659\n",
      "Epoch 291/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0898 - accuracy: 0.9657\n",
      "Epoch 292/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0860 - accuracy: 0.9682\n",
      "Epoch 293/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0830 - accuracy: 0.9699\n",
      "Epoch 294/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0836 - accuracy: 0.9690\n",
      "Epoch 295/300\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0846 - accuracy: 0.9686\n",
      "Epoch 296/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0855 - accuracy: 0.9694\n",
      "Epoch 297/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0853 - accuracy: 0.9669\n",
      "Epoch 298/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0827 - accuracy: 0.9720\n",
      "Epoch 299/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1292 - accuracy: 0.9510\n",
      "Epoch 300/300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.2207 - accuracy: 0.9221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c84acd110>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(one_hot_encoded, one_hot_encoded, epochs=300, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bY5aTbgB6YX"
   },
   "outputs": [],
   "source": [
    "# Recosntruct passwords through autoencoder as vectors\n",
    "reconst_passwd_vecs = autoencoder.predict(one_hot_encoded)\n",
    "# Reverse one hot encoding to covnert passwords to strings\n",
    "unpad = lambda text: text.replace(PAD_CHAR, \"\")\n",
    "one_hot_decode = lambda one_hot_vectors: \"\".join([list(charset)[np.argmax(vec)] for vec in one_hot_vectors])\n",
    "reconst_passwd_str = [unpad(one_hot_decode(p)) for p in reconst_passwd_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "Ewkv-7rkCLWp",
    "outputId": "d70f0a66-1c70-48cd-9234-b9032432e67d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Password</th>\n",
       "      <th>Recosntructed Password</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acafe2019!</td>\n",
       "      <td>Pcafe2019!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahyper2019!</td>\n",
       "      <td>Ahyper2019!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pleet10!</td>\n",
       "      <td>Eleet10!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ababe2!</td>\n",
       "      <td>Ababe2!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tbingo1234*</td>\n",
       "      <td>Tbingo1234*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eleet2019@</td>\n",
       "      <td>Pleet2019@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tninja123!</td>\n",
       "      <td>Pninja123!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aninja777*</td>\n",
       "      <td>Aninja777@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ababe101*</td>\n",
       "      <td>Ababe101*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tbingo2019_</td>\n",
       "      <td>Tbingo2019*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original Password Recosntructed Password\n",
       "0        Acafe2019!             Pcafe2019!\n",
       "1       Ahyper2019!            Ahyper2019!\n",
       "2          Pleet10!               Eleet10!\n",
       "3           Ababe2!                Ababe2!\n",
       "4       Tbingo1234*            Tbingo1234*\n",
       "5        Eleet2019@             Pleet2019@\n",
       "6        Tninja123!             Pninja123!\n",
       "7        Aninja777*             Aninja777@\n",
       "8         Ababe101*              Ababe101*\n",
       "9       Tbingo2019_            Tbingo2019*"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare original vs reconstructed passwords\n",
    "passwords_df = pd.DataFrame(zip(passwords[\"FullPassword\"], reconst_passwd_str),\n",
    "                            columns = ['Original Password', 'Recosntructed Password'])\n",
    "passwords_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "yPpG6YMzoUpI",
    "outputId": "3ba41650-f795-4e37-e088-a514472f8adc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Password</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahackerrnn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thycerr2$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ehhccckaaaaaaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahyneerman001*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ehackkrrman0@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pningrr1118$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ahaee777!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phaccca1a1**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kpyppp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eh4ceermmmmm1@</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Password\n",
       "0      Ahackerrnn0\n",
       "1        Thycerr2$\n",
       "2  Ehhccckaaaaaaaa\n",
       "3   Ahyneerman001*\n",
       "4    Ehackkrrman0@\n",
       "5     Pningrr1118$\n",
       "6        Ahaee777!\n",
       "7     Phaccca1a1**\n",
       "8           kpyppp\n",
       "9   Eh4ceermmmmm1@"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Feel free to change the latent space values as you like and explore what comes\n",
    "# out from the decoder\n",
    "new_passwords = []\n",
    "for _ in range(1000):\n",
    "  latent_sample = np.array([[random.uniform(-1, 1) for _ in range(6)]])\n",
    "  new_password_vec = decoder.predict(latent_sample)\n",
    "  new_password_str = unpad(one_hot_decode(new_password_vec[0]))\n",
    "  new_passwords.append(new_password_str)\n",
    "new_passwords_df = pd.DataFrame(new_passwords, columns=[\"Password\"])\n",
    "\n",
    "# Save them into a CSV file\n",
    "new_passwords_df.to_csv(path + 'data/output/ae_sample_pass.csv', sep=',')\n",
    "\n",
    "new_passwords_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte the Average Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZCwvp5Xiq2u",
    "outputId": "714d99ed-0437-44af-a7e9-59d940910eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BiEntropy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/6e/fe3aa825f472b473d8b70268bad4dc9cf1a5243503e28ae5759dfe23690d/BiEntropy-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (155kB)\n",
      "\r",
      "\u001b[K     |██                              | 10kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 20kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 30kB 14.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 40kB 13.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 51kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 61kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 71kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 81kB 9.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 92kB 9.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 102kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 112kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 122kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 133kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 143kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 153kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 163kB 8.6MB/s \n",
      "\u001b[?25hCollecting bitstring>=3.1.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fc/ffac2c199d2efe1ec5111f55efeb78f5f2972456df6939fea849f103f9f5/bitstring-3.1.7.tar.gz (195kB)\n",
      "\r",
      "\u001b[K     |█▊                              | 10kB 27.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 20kB 36.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 30kB 31.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 40kB 21.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 51kB 17.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 61kB 19.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 71kB 17.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 81kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 92kB 14.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 102kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 112kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 122kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 133kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 143kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 153kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 163kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 174kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 184kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 194kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 204kB 14.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from BiEntropy) (1.19.5)\n",
      "Building wheels for collected packages: bitstring\n",
      "  Building wheel for bitstring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bitstring: filename=bitstring-3.1.7-cp37-none-any.whl size=37949 sha256=816e7f8d752347acd5ceb9d32ba1f84111c1092e4cc6ca101225ff2a22ca5022\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/27/f0/8373e26b7de57db03dc18aaaebdd8c26a99da882416f762979\n",
      "Successfully built bitstring\n",
      "Installing collected packages: bitstring, BiEntropy\n",
      "Successfully installed BiEntropy-1.1.4 bitstring-3.1.7\n",
      "MAX entropy: 0.9769249653966429\n",
      "MIN entropy: 0.18276011443364695\n",
      "AVG entropy: 0.9060250326312096\n"
     ]
    }
   ],
   "source": [
    "!pip install BiEntropy\n",
    "\n",
    "from bientropy import bien, tbien\n",
    "\n",
    "sum_entropy = 0\n",
    "max_entropy = float('-inf')\n",
    "min_entropy = float('inf')\n",
    "\n",
    "for pswd in new_passwords:\n",
    "  pswd_bytes = bytes(pswd, 'utf-8')\n",
    "  e = tbien(pswd_bytes)\n",
    "  sum_entropy += e\n",
    "  # update the max and the min entropy\n",
    "  max_entropy = max(max_entropy, e)\n",
    "  min_entropy = min(min_entropy, e)\n",
    "\n",
    "avg_entropy = sum_entropy / len(new_passwords)\n",
    "\n",
    "print(\"MAX entropy: \" + str(max_entropy))\n",
    "print(\"MIN entropy: \" + str(min_entropy))\n",
    "print(\"AVG entropy: \" + str(avg_entropy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1kPass_Standard_Autoencoder_Generating_Passwords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
